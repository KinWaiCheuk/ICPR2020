{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT filter created, time used = 0.2081 seconds\n",
      "Mel filter created, time used = 0.0051 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sacred import Experiment\n",
    "from sacred.commands import print_config\n",
    "from sacred.observers import FileStorageObserver\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "from evaluate import evaluate, evaluate_wo_velocity # These two lines requires GPU\n",
    "from onsets_and_frames import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading group AkPnBcht: 100%|██████████| 30/30 [00:00<00:00, 291.29it/s]\n",
      "Loading group AkPnBsdf:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 7 groups of MAPS at data/MAPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading group AkPnBsdf: 100%|██████████| 30/30 [00:00<00:00, 253.12it/s]\n",
      "Loading group AkPnCGdD: 100%|██████████| 30/30 [00:00<00:00, 271.50it/s]\n",
      "Loading group AkPnStgb: 100%|██████████| 30/30 [00:00<00:00, 199.04it/s]\n",
      "Loading group SptkBGAm: 100%|██████████| 30/30 [00:00<00:00, 299.94it/s]\n",
      "Loading group SptkBGCl: 100%|██████████| 30/30 [00:00<00:00, 219.53it/s]\n",
      "Loading group StbgTGd2: 100%|██████████| 30/30 [00:00<00:00, 208.81it/s]\n",
      "Loading group ENSTDkAm: 100%|██████████| 30/30 [00:00<00:00, 226.55it/s]\n",
      "Loading group ENSTDkCl:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2 groups of MAPS at data/MAPS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading group ENSTDkCl: 100%|██████████| 30/30 [00:00<00:00, 208.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OnsetsAndFrames(\n",
      "  (onset_stack): Sequential(\n",
      "    (0): ConvStack(\n",
      "      (cnn): Sequential(\n",
      "        (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m480\u001b[0m params\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (2): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m20,784\u001b[0m params\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (5): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (7): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "        (8): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m41,568\u001b[0m params\n",
      "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m192\u001b[0m params\n",
      "        (10): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (12): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m63,216\u001b[0m params\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=5472, out_features=768, bias=True), \u001b[92m4,203,264\u001b[0m params\n",
      "        (1): Dropout(p=0.5, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m4,203,264\u001b[0m params\n",
      "    ), \u001b[92m4,266,480\u001b[0m params\n",
      "    (1): BiLSTM(\n",
      "      (rnn): LSTM(768, 384, batch_first=True, bidirectional=True), \u001b[92m3,545,088\u001b[0m params\n",
      "    ), \u001b[92m3,545,088\u001b[0m params\n",
      "    (2): Linear(in_features=768, out_features=88, bias=True), \u001b[92m67,672\u001b[0m params\n",
      "    (3): Sigmoid(), \u001b[92m0\u001b[0m params\n",
      "  ), \u001b[92m7,879,240\u001b[0m params\n",
      "  (offset_stack): Sequential(\n",
      "    (0): ConvStack(\n",
      "      (cnn): Sequential(\n",
      "        (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m480\u001b[0m params\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (2): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m20,784\u001b[0m params\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (5): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (7): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "        (8): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m41,568\u001b[0m params\n",
      "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m192\u001b[0m params\n",
      "        (10): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (12): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m63,216\u001b[0m params\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=5472, out_features=768, bias=True), \u001b[92m4,203,264\u001b[0m params\n",
      "        (1): Dropout(p=0.5, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m4,203,264\u001b[0m params\n",
      "    ), \u001b[92m4,266,480\u001b[0m params\n",
      "    (1): BiLSTM(\n",
      "      (rnn): LSTM(768, 384, batch_first=True, bidirectional=True), \u001b[92m3,545,088\u001b[0m params\n",
      "    ), \u001b[92m3,545,088\u001b[0m params\n",
      "    (2): Linear(in_features=768, out_features=88, bias=True), \u001b[92m67,672\u001b[0m params\n",
      "    (3): Sigmoid(), \u001b[92m0\u001b[0m params\n",
      "  ), \u001b[92m7,879,240\u001b[0m params\n",
      "  (frame_stack): Sequential(\n",
      "    (0): ConvStack(\n",
      "      (cnn): Sequential(\n",
      "        (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m480\u001b[0m params\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (2): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m20,784\u001b[0m params\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (5): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (7): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "        (8): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m41,568\u001b[0m params\n",
      "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m192\u001b[0m params\n",
      "        (10): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (12): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m63,216\u001b[0m params\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=5472, out_features=768, bias=True), \u001b[92m4,203,264\u001b[0m params\n",
      "        (1): Dropout(p=0.5, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m4,203,264\u001b[0m params\n",
      "    ), \u001b[92m4,266,480\u001b[0m params\n",
      "    (1): Linear(in_features=768, out_features=88, bias=True), \u001b[92m67,672\u001b[0m params\n",
      "    (2): Sigmoid(), \u001b[92m0\u001b[0m params\n",
      "  ), \u001b[92m4,334,152\u001b[0m params\n",
      "  (combined_stack): Sequential(\n",
      "    (0): BiLSTM(\n",
      "      (rnn): LSTM(176, 384, batch_first=True, bidirectional=True), \u001b[92m1,726,464\u001b[0m params\n",
      "    ), \u001b[92m1,726,464\u001b[0m params\n",
      "    (1): Linear(in_features=768, out_features=88, bias=True), \u001b[92m67,672\u001b[0m params\n",
      "    (2): Sigmoid(), \u001b[92m0\u001b[0m params\n",
      "  ), \u001b[92m1,794,136\u001b[0m params\n",
      "  (velocity_stack): Sequential(\n",
      "    (0): ConvStack(\n",
      "      (cnn): Sequential(\n",
      "        (0): Conv2d(1, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m480\u001b[0m params\n",
      "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (2): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m20,784\u001b[0m params\n",
      "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m96\u001b[0m params\n",
      "        (5): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (6): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (7): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "        (8): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), \u001b[92m41,568\u001b[0m params\n",
      "        (9): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), \u001b[92m192\u001b[0m params\n",
      "        (10): ReLU(), \u001b[92m0\u001b[0m params\n",
      "        (11): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False), \u001b[92m0\u001b[0m params\n",
      "        (12): Dropout(p=0.25, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m63,216\u001b[0m params\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=5472, out_features=768, bias=True), \u001b[92m4,203,264\u001b[0m params\n",
      "        (1): Dropout(p=0.5, inplace=False), \u001b[92m0\u001b[0m params\n",
      "      ), \u001b[92m4,203,264\u001b[0m params\n",
      "    ), \u001b[92m4,266,480\u001b[0m params\n",
      "    (1): Linear(in_features=768, out_features=88, bias=True), \u001b[92m67,672\u001b[0m params\n",
      "  ), \u001b[92m4,334,152\u001b[0m params\n",
      "), \u001b[92m26,220,920\u001b[0m params\n"
     ]
    }
   ],
   "source": [
    "logdir = 'runs/transcriber-' + datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "iterations = 500000\n",
    "resume_iteration = None\n",
    "checkpoint_interval = 1000\n",
    "train_on = 'MAPS'\n",
    "\n",
    "batch_size = 8\n",
    "sequence_length = 327680\n",
    "model_complexity = 48\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_properties(torch.cuda.current_device()).total_memory < 10e9:\n",
    "    batch_size //= 2\n",
    "    sequence_length //= 2\n",
    "    print(f'Reducing batch size to {batch_size} and sequence_length to {sequence_length} to save memory')\n",
    "\n",
    "learning_rate = 0.0006\n",
    "learning_rate_decay_steps = 10000\n",
    "learning_rate_decay_rate = 0.98\n",
    "\n",
    "leave_one_out = None\n",
    "\n",
    "clip_gradient_norm = 3\n",
    "\n",
    "validation_length = sequence_length\n",
    "validation_interval = 500\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "train_groups, validation_groups = ['train'], ['validation']\n",
    "\n",
    "if leave_one_out is not None:\n",
    "    all_years = {'2004', '2006', '2008', '2009', '2011', '2013', '2014', '2015', '2017'}\n",
    "    train_groups = list(all_years - {str(leave_one_out)})\n",
    "    validation_groups = [str(leave_one_out)]\n",
    "\n",
    "if train_on == 'MAESTRO':\n",
    "    dataset = MAESTRO(groups=train_groups, sequence_length=sequence_length)\n",
    "    validation_dataset = MAESTRO(groups=validation_groups, sequence_length=sequence_length)\n",
    "else:\n",
    "    dataset = MAPS(groups=['AkPnBcht', 'AkPnBsdf', 'AkPnCGdD', 'AkPnStgb', 'SptkBGAm', 'SptkBGCl', 'StbgTGd2'], sequence_length=sequence_length)\n",
    "    validation_dataset = MAPS(groups=['ENSTDkAm', 'ENSTDkCl'], sequence_length=validation_length)\n",
    "\n",
    "loader = DataLoader(dataset, batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "if resume_iteration is None:\n",
    "    model = OnsetsAndFrames(N_MELS, MAX_MIDI - MIN_MIDI + 1, model_complexity).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    resume_iteration = 0\n",
    "else:\n",
    "    model_path = os.path.join(logdir, f'model-{resume_iteration}.pt')\n",
    "    model = torch.load(model_path)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(logdir, 'last-optimizer-state.pt')))\n",
    "\n",
    "summary(model)\n",
    "scheduler = StepLR(optimizer, step_size=learning_rate_decay_steps, gamma=learning_rate_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = evaluate_wo_velocity(validation_dataset, model).items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frame'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'frame'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chroma_total_error'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chroma_total_error'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'f1' in 'note f1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            note precision                : 0.000 ± 0.000\n",
      "                            note recall                   : 0.000 ± 0.000\n",
      "                            note f1                       : 0.000 ± 0.000\n",
      "               note-with-offsets precision                : 0.000 ± 0.000\n",
      "               note-with-offsets recall                   : 0.000 ± 0.000\n",
      "               note-with-offsets f1                       : 0.000 ± 0.000\n",
      "                           frame f1                       : 0.000 ± 0.000\n",
      "                           frame precision                : 0.000 ± 0.000\n",
      "                           frame recall                   : 0.000 ± 0.000\n"
     ]
    }
   ],
   "source": [
    "for key, values in eval_dict:\n",
    "    if key.startswith('metric/'):\n",
    "        _, category, name = key.split('/')\n",
    "        if ('precision' in name or 'recall' in name or 'f1' in name) and 'chroma' not in name:\n",
    "            print(f'{category:>32} {name:25}: {np.mean(values):.3f} ± {np.std(values):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\tLoss: 0.362543                                                                       \n",
      "Train Epoch: 2\tLoss: 0.156357                                                                       \n",
      "Train Epoch: 3\tLoss: 0.155357                                                                       \n",
      "Train Epoch: 4\tLoss: 0.154539                                                                       \n",
      "Train Epoch: 5\tLoss: 0.151398                                                                       \n",
      "Train Epoch: 6 [120/210(57%)]\tLoss: 0.149465\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-99ba0df99a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop = tqdm(range(resume_iteration + 1, iterations + 1))\n",
    "epoches = 100\n",
    "total_batch = len(loader.dataset)\n",
    "for ep in range(1, epoches):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_idx = 0\n",
    "    for batch in loader:\n",
    "        predictions, losses = model.run_on_batch(batch)\n",
    "\n",
    "        loss = sum(losses.values())\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if clip_gradient_norm:\n",
    "            clip_grad_norm_(model.parameters(), clip_gradient_norm)\n",
    "        batch_idx += 1\n",
    "        print(f'Train Epoch: {ep} [{batch_idx*batch_size}/{total_batch}'\n",
    "                f'({100. * batch_idx*batch_size / total_batch:.0f}%)]'\n",
    "                f'\\tLoss: {loss.item():.6f}'\n",
    "                , end='\\r') \n",
    "    print(' '*100, end = '\\r')            \n",
    "    print(f'Train Epoch: {ep}\\tLoss: {total_loss/len(loader):.6f}')\n",
    "\n",
    "    if ep%10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for key, values in evaluate_wo_velocity(validation_dataset, model).items():\n",
    "                if key.startswith('metric/'):\n",
    "                        _, category, name = key.split('/')\n",
    "                        print(f'{category:>32} {name:25}: {np.mean(values):.3f} ± {np.std(values):.3f}')\n",
    "\n",
    "\n",
    "    # for key, value in {'loss': loss, **losses}.items():\n",
    "    #     writer.add_scalar(key, value.item(), global_step=i)\n",
    "\n",
    "    # if i % validation_interval == 0:\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         for key, value in evaluate(validation_dataset, model).items():\n",
    "    #             writer.add_scalar('validation/' + key.replace(' ', '_'), np.mean(value), global_step=i)\n",
    "    #     model.train()\n",
    "\n",
    "    # if i % checkpoint_interval == 0:\n",
    "    #     torch.save(model, os.path.join(logdir, f'model-{i}.pt'))\n",
    "    #     torch.save(optimizer.state_dict(), os.path.join(logdir, 'last-optimizer-state.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in {'loss': loss, **losses}.items():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'loss': loss, **losses}.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(0.1346, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " 'loss/onset': tensor(0.0255, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>),\n",
       " 'loss/frame': tensor(0.1091, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'loss': loss, **losses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss/onset': tensor(0.0255, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>),\n",
       " 'loss/frame': tensor(0.1091, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss/onset\n",
      "loss/frame\n"
     ]
    }
   ],
   "source": [
    "for value in losses:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
